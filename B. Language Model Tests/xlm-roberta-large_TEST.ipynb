{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "363d427d-f02e-4446-a360-c186a16b5a77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b22c71ac6954f6180faaf211e2d5405",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python\\Lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\SGhanbariHaez\\.cache\\huggingface\\hub\\models--xlm-roberta-large. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17e5027c53fa419d941ea889f9fdf374",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f587a01623b04569b006883b92f241ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.10M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9b531d031194771945cbd2e3fc6ebce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/616 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b55941746d82455fb195768ccb4c0825",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.24G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForMultipleChoice were not initialized from the model checkpoint at xlm-roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [1/60] Saved: B\n",
      "✅ [2/60] Saved: A\n",
      "✅ [3/60] Saved: D\n",
      "✅ [4/60] Saved: D\n",
      "✅ [5/60] Saved: C\n",
      "✅ [6/60] Saved: B\n",
      "✅ [7/60] Saved: B\n",
      "✅ [8/60] Saved: B\n",
      "✅ [9/60] Saved: B\n",
      "✅ [10/60] Saved: B\n",
      "✅ [11/60] Saved: B\n",
      "✅ [12/60] Saved: B\n",
      "✅ [13/60] Saved: D\n",
      "✅ [14/60] Saved: D\n",
      "✅ [15/60] Saved: B\n",
      "✅ [16/60] Saved: B\n",
      "✅ [17/60] Saved: B\n",
      "✅ [18/60] Saved: C\n",
      "✅ [19/60] Saved: C\n",
      "✅ [20/60] Saved: A\n",
      "✅ [21/60] Saved: C\n",
      "✅ [22/60] Saved: D\n",
      "✅ [23/60] Saved: D\n",
      "✅ [24/60] Saved: B\n",
      "✅ [25/60] Saved: B\n",
      "✅ [26/60] Saved: B\n",
      "✅ [27/60] Saved: A\n",
      "✅ [28/60] Saved: B\n",
      "✅ [29/60] Saved: B\n",
      "✅ [30/60] Saved: D\n",
      "✅ [31/60] Saved: C\n",
      "✅ [32/60] Saved: B\n",
      "✅ [33/60] Saved: A\n",
      "✅ [34/60] Saved: D\n",
      "✅ [35/60] Saved: B\n",
      "✅ [36/60] Saved: C\n",
      "✅ [37/60] Saved: D\n",
      "✅ [38/60] Saved: B\n",
      "✅ [39/60] Saved: B\n",
      "✅ [40/60] Saved: B\n",
      "✅ [41/60] Saved: B\n",
      "✅ [42/60] Saved: D\n",
      "✅ [43/60] Saved: A\n",
      "✅ [44/60] Saved: B\n",
      "✅ [45/60] Saved: D\n",
      "✅ [46/60] Saved: A\n",
      "✅ [47/60] Saved: B\n",
      "✅ [48/60] Saved: D\n",
      "✅ [49/60] Saved: A\n",
      "✅ [50/60] Saved: B\n",
      "✅ [51/60] Saved: B\n",
      "✅ [52/60] Saved: B\n",
      "✅ [53/60] Saved: D\n",
      "✅ [54/60] Saved: B\n",
      "✅ [55/60] Saved: D\n",
      "✅ [56/60] Saved: C\n",
      "✅ [57/60] Saved: B\n",
      "✅ [58/60] Saved: B\n",
      "✅ [59/60] Saved: B\n",
      "✅ [60/60] Saved: A\n",
      "✅ [1/60] Saved: B\n",
      "✅ [2/60] Saved: C\n",
      "✅ [3/60] Saved: D\n",
      "✅ [4/60] Saved: A\n",
      "✅ [5/60] Saved: A\n",
      "✅ [6/60] Saved: B\n",
      "✅ [7/60] Saved: A\n",
      "✅ [8/60] Saved: A\n",
      "✅ [9/60] Saved: C\n",
      "✅ [10/60] Saved: A\n",
      "✅ [11/60] Saved: C\n",
      "✅ [12/60] Saved: A\n",
      "✅ [13/60] Saved: C\n",
      "✅ [14/60] Saved: D\n",
      "✅ [15/60] Saved: D\n",
      "✅ [16/60] Saved: A\n",
      "✅ [17/60] Saved: B\n",
      "✅ [18/60] Saved: D\n",
      "✅ [19/60] Saved: C\n",
      "✅ [20/60] Saved: C\n",
      "✅ [21/60] Saved: A\n",
      "✅ [22/60] Saved: C\n",
      "✅ [23/60] Saved: D\n",
      "✅ [24/60] Saved: B\n",
      "✅ [25/60] Saved: B\n",
      "✅ [26/60] Saved: A\n",
      "✅ [27/60] Saved: B\n",
      "✅ [28/60] Saved: C\n",
      "✅ [29/60] Saved: C\n",
      "✅ [30/60] Saved: C\n",
      "✅ [31/60] Saved: C\n",
      "✅ [32/60] Saved: B\n",
      "✅ [33/60] Saved: B\n",
      "✅ [34/60] Saved: A\n",
      "✅ [35/60] Saved: D\n",
      "✅ [36/60] Saved: C\n",
      "✅ [37/60] Saved: A\n",
      "✅ [38/60] Saved: B\n",
      "✅ [39/60] Saved: C\n",
      "✅ [40/60] Saved: A\n",
      "✅ [41/60] Saved: C\n",
      "✅ [42/60] Saved: C\n",
      "✅ [43/60] Saved: C\n",
      "✅ [44/60] Saved: B\n",
      "✅ [45/60] Saved: C\n",
      "✅ [46/60] Saved: C\n",
      "✅ [47/60] Saved: A\n",
      "✅ [48/60] Saved: B\n",
      "✅ [49/60] Saved: B\n",
      "✅ [50/60] Saved: A\n",
      "✅ [51/60] Saved: B\n",
      "✅ [52/60] Saved: D\n",
      "✅ [53/60] Saved: A\n",
      "✅ [54/60] Saved: D\n",
      "✅ [55/60] Saved: D\n",
      "✅ [56/60] Saved: A\n",
      "✅ [57/60] Saved: D\n",
      "✅ [58/60] Saved: A\n",
      "✅ [59/60] Saved: A\n",
      "✅ [60/60] Saved: C\n",
      "✅ [1/60] Saved: C\n",
      "✅ [2/60] Saved: A\n",
      "✅ [3/60] Saved: A\n",
      "✅ [4/60] Saved: B\n",
      "✅ [5/60] Saved: A\n",
      "✅ [6/60] Saved: B\n",
      "✅ [7/60] Saved: C\n",
      "✅ [8/60] Saved: B\n",
      "✅ [9/60] Saved: A\n",
      "✅ [10/60] Saved: A\n",
      "✅ [11/60] Saved: A\n",
      "✅ [12/60] Saved: B\n",
      "✅ [13/60] Saved: A\n",
      "✅ [14/60] Saved: B\n",
      "✅ [15/60] Saved: A\n",
      "✅ [16/60] Saved: C\n",
      "✅ [17/60] Saved: B\n",
      "✅ [18/60] Saved: A\n",
      "✅ [19/60] Saved: A\n",
      "✅ [20/60] Saved: A\n",
      "✅ [21/60] Saved: A\n",
      "✅ [22/60] Saved: B\n",
      "✅ [23/60] Saved: B\n",
      "✅ [24/60] Saved: B\n",
      "✅ [25/60] Saved: B\n",
      "✅ [26/60] Saved: A\n",
      "✅ [27/60] Saved: A\n",
      "✅ [28/60] Saved: A\n",
      "✅ [29/60] Saved: B\n",
      "✅ [30/60] Saved: C\n",
      "✅ [31/60] Saved: A\n",
      "✅ [32/60] Saved: A\n",
      "✅ [33/60] Saved: B\n",
      "✅ [34/60] Saved: A\n",
      "✅ [35/60] Saved: B\n",
      "✅ [36/60] Saved: B\n",
      "✅ [37/60] Saved: B\n",
      "✅ [38/60] Saved: A\n",
      "✅ [39/60] Saved: C\n",
      "✅ [40/60] Saved: C\n",
      "✅ [41/60] Saved: B\n",
      "✅ [42/60] Saved: B\n",
      "✅ [43/60] Saved: B\n",
      "✅ [44/60] Saved: A\n",
      "✅ [45/60] Saved: B\n",
      "✅ [46/60] Saved: A\n",
      "✅ [47/60] Saved: C\n",
      "✅ [48/60] Saved: A\n",
      "✅ [49/60] Saved: B\n",
      "✅ [50/60] Saved: D\n",
      "✅ [51/60] Saved: A\n",
      "✅ [52/60] Saved: D\n",
      "✅ [53/60] Saved: C\n",
      "✅ [54/60] Saved: A\n",
      "✅ [55/60] Saved: D\n",
      "✅ [56/60] Saved: A\n",
      "✅ [57/60] Saved: B\n",
      "✅ [58/60] Saved: B\n",
      "✅ [59/60] Saved: C\n",
      "✅ [60/60] Saved: A\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "from transformers import XLMRobertaTokenizer, XLMRobertaForMultipleChoice\n",
    "import os\n",
    "\n",
    "# Load tokenizer and model\n",
    "model_name = \"xlm-roberta-large\"\n",
    "tokenizer = XLMRobertaTokenizer.from_pretrained(model_name)\n",
    "model = XLMRobertaForMultipleChoice.from_pretrained(model_name)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "def predict_choice(prompt, options):\n",
    "    # Prepare input for multiple choice\n",
    "    choices_inputs = tokenizer(\n",
    "        [f\"{prompt} {opt}\" for opt in options],\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    for k in choices_inputs:\n",
    "        choices_inputs[k] = choices_inputs[k].unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**choices_inputs)\n",
    "        prediction = torch.argmax(outputs.logits, dim=1).item()\n",
    "        return [\"A\", \"B\", \"C\", \"D\"][prediction]\n",
    "\n",
    "def process_json_file(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    for i, item in enumerate(data):\n",
    "        try:\n",
    "            prompt = item[\"prompt\"].replace(\"[adj and adj]\", \"[ ]\")\n",
    "            options = [item[\"options\"][\"A\"], item[\"options\"][\"B\"], item[\"options\"][\"C\"], item[\"options\"][\"D\"]]\n",
    "            choice = predict_choice(prompt, options)\n",
    "        except Exception as e:\n",
    "            print(f\"Error at index {i}: {e}\")\n",
    "            choice = \"Error\"\n",
    "\n",
    "        item[\"XLM-R Choice\"] = choice\n",
    "\n",
    "        with open(file_path.replace(\".json\", \"_xlmr_output.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "        print(f\"✅ [{i+1}/{len(data)}] Saved: {choice}\")\n",
    "\n",
    "# Process all 3 files\n",
    "for filename in [\n",
    "    \"multiple_choice_prompts_Low.json\",\n",
    "    \"multiple_choice_prompts_Mid.json\",\n",
    "    \"multiple_choice_prompts_High.json\"\n",
    "]:\n",
    "    process_json_file(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bbb36f-1761-4f2c-9b17-c97b9f9c7a81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
