# Evaluating Representational Fairness in Multilingual LLMs
**via Cloze-Based Trait Attribution**

> Grounded in Schwartzâ€™s theory of human values, we evaluate whether multilingual LLMs preserve
> valueâ€‘oriented trait attributions across languages using a controlled cloze task.

---

## ðŸ§© Abstract
As large language models become integral to cross-lingual applications, it is critical to assess how they represent
social roles and values across cultures. Grounded in Schwartzâ€™s theory of human values, we represent an evaluation
framework using cloze-style prompts to measure value-based trait attribution across English, Italian, and Persian.
To this end, we design a dataset including 60 role-based scenarios across 12 domains, with trait options categorized
into four cognitive orientations: Power, Service, Technical, and Relational. We apply this framework to six
modelsâ€”four multilingual decoder-based LLMs (GPT-4o, LLaMA 3.1:70B, LLaMA 3.2, Qwen2.5:14B) and two
masked language models (XLM-R, mBERT)â€”and analyze cross-lingual agreement, orientation preferences, and
framing divergence. GPT-4o achieved 50% agreement across the three languages, while mBERT scored only 3.3%,
revealing sharp disparities in semantic consistency. Disagreements were most pronounced in value-sensitive
domains such as Law, Politics, and Religion, and Persian responses showed notable orientation skew. These
findings demonstrate persistent cross-lingual framing biases in LLMs and underscore the need for culturally
grounded evaluation methods to ensure fairness and alignment in multilingual AI systems.

---

## ðŸ“ Repository Layout
```
Cross-Lingual Bias Detection/
â”œâ”€ A. Datasets/                # EN(High), IT(Mid), FA(Low) + shuffled variants
â”œâ”€ B. Language Model Tests/    # Runs for GPT-4o, LLaMA, Qwen, mBERT, XLM-R
â”œâ”€ C. Merging LM Responses/    # Merge EN/IT/FA outputs per model
â”œâ”€ D. Analysis/                # Agreement %, disagreement types, domains, entropy, CIs
â””â”€ README.md                   # This landing page
```

Each subfolder can include its own `README.md` with details (recommended).

---

## ðŸš€ Quickstart
```bash
# Python 3.9+ recommended
pip install -U pandas numpy torch transformers openai

# 1) Run models on the datasets
# GPTâ€‘4o: add OpenAI key to B. Language Model Tests/secret.py as: apikey = "YOUR_KEY"
python "B. Language Model Tests/GPT4o_TEST.py"

# 2) Merge outputs (align EN/IT/FA)
python "C. Merging LM Responses/merge_gpt4o.py"

# 3) Analyze agreement/disagreement/entropy
python "D. Analysis/analyze_gpt4o.py"
```

> Shuffled orderâ€‘effect scripts are included for GPTâ€‘4o and LLaMAâ€‘3.1â€‘70B.

---

## ðŸ§ª Notes
- Prompts use a placeholder `[ ]` and `temperature=0` for deterministic decoding.
- Scripts autoâ€‘resume if interrupted and save perâ€‘row progress.
- Consider Git LFS if any CSVs are very large.

---

## ðŸ“¬ Contact
Add your name/email here. Paper link will be added upon publication.
